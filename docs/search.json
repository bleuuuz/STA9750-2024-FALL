[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Bleuuuz",
    "section": "",
    "text": "Bleuuuz is an Masters student studying Quantitative Methods and Modeling at theZicklin School of Business, Baruch College, CUNY. Before joining Baruch, he worked at JPMorgan and Bank of America."
  },
  {
    "objectID": "Houston Housing.html",
    "href": "Houston Housing.html",
    "title": "Houston Housing",
    "section": "",
    "text": "if(!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)\ntxhousing |&gt; filter(city==\"Houston\") |&gt; \n             group_by(year) |&gt; \n             summarize(sales=sum(sales)) |&gt; \n             ggplot(aes(x=year, y=sales)) + \n                geom_line() + \n                ggtitle(\"Annual Houses Sold in Houston, TX\")\n\n\n\n\n\n\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\ntxhousing |&gt; filter(city==\"Houston\") |&gt; \n    group_by(month) |&gt; \n    summarize(avg_price=sum(volume) / sum(sales)) |&gt; \n    mutate(month=factor(month.abb[month], levels=month.abb, ordered=TRUE)) |&gt;\n    ggplot(aes(x=month, y=avg_price)) + \n    geom_bar(stat=\"identity\") + \n    ggtitle(\"Average Price of Houses Sold in Texas by Month\") + \n    xlab(\"Month\") + \n    ylab(\"Average Sale Price\") + \n    scale_y_continuous(labels = scales::dollar)\n\n\n\n\n\n\n\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\ntxhousing |&gt; filter(year==2015) |&gt; \n    group_by(city) |&gt; \n    summarize(avg_price=sum(volume) / sum(sales),\n              num_sales=sum(sales)) |&gt; \n    slice_max(num_sales, n=10) |&gt;\n    ggplot(aes(x=city, y=avg_price)) + \n    geom_bar(stat=\"identity\") + \n    ggtitle(\"Average Price of Houses Sold in 2015 by City in Texas\") + \n    xlab(\"City\") + \n    ylab(\"Average Sale Price\") + \n    scale_y_continuous(labels = scales::dollar)"
  },
  {
    "objectID": "Houston Housing Presentation.html#houston-housing-market",
    "href": "Houston Housing Presentation.html#houston-housing-market",
    "title": "Houston Housing",
    "section": "Houston Housing Market",
    "text": "Houston Housing Market"
  },
  {
    "objectID": "Houston Housing Presentation.html#annual-houses-sold-in-houston-tx",
    "href": "Houston Housing Presentation.html#annual-houses-sold-in-houston-tx",
    "title": "Houston Housing",
    "section": "Annual Houses Sold in Houston, TX",
    "text": "Annual Houses Sold in Houston, TX\nOver the time period from 2000-2015 we have seen volatile housing sales ranging from just under 50k a year up to 83k."
  },
  {
    "objectID": "Houston Housing Presentation.html#average-price-of-houses-sold-in-texas-by-month",
    "href": "Houston Housing Presentation.html#average-price-of-houses-sold-in-texas-by-month",
    "title": "Houston Housing",
    "section": "Average Price of Houses Sold in Texas by Month",
    "text": "Average Price of Houses Sold in Texas by Month\nDespite the volatile housing sales, we have seen the average price of houses remain relatively similar throughout different months of the year."
  },
  {
    "objectID": "Houston Housing Presentation.html#average-price-of-houses-sold-in-2015-by-city-in-texas",
    "href": "Houston Housing Presentation.html#average-price-of-houses-sold-in-2015-by-city-in-texas",
    "title": "Houston Housing",
    "section": "Average Price of Houses Sold in 2015 by City in Texas",
    "text": "Average Price of Houses Sold in 2015 by City in Texas\nIn 2015, Collin County saw the highest average house price north of $300k a house compared to Fort Worth with the lowest at just under $200k."
  },
  {
    "objectID": "Houston Housing Presentation.html#conclusion",
    "href": "Houston Housing Presentation.html#conclusion",
    "title": "Houston Housing",
    "section": "Conclusion",
    "text": "Conclusion\nTexas is not as expensive as New York"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Introducing the United States’ Public Transit Network: Your Journey, Your Delay",
    "section": "",
    "text": "From the bustling streets of New York City to the serene landscapes of the Pacific Northwest, America’s public transit systems offer a diverse range of options to get you where you need to go… eventually.\nIf the American transit systems were to have a commercial brochure it would look a little like the following:\nExperience the convenience and efficiency of American Public Transit with…\n\nSubways: Navigate the heart of major cities with ease and speed… or watch as the train you were running to catch closes its doors in front of your eyes.\nBuses: Explore neighborhoods and connect to popular destinations… as long as they haven’t been rerouted due to unexpected construction or traffic.\nTrains: Enjoy scenic journeys and efficient transportation between cities… unless there’s a mechanical issue or a signal malfunction.\nLight rail: Experience modern, eco-friendly travel in urban areas… just be prepared to wait half an hour for the next one to arrive.\nStreetcars: Discover historic charm and convenient transportation options… or experience the occasional track fire.\n\nAlthough the US transit systems are prone to issues as highlighted in the free brochure, they also serve as a ‘reliable’, accessible, and affordable means of transportation. (Unless you live in NYC where they just hiked fares to $2.90)\nAs I learned more about this topic, I began to realize just how far-reaching the US transit system was. I mean, did you know that within the United States, there are 678 different transit systems? I would’ve never guessed.\nDelving deeper into this train of thought, I pulled together data from 3 different sources on the National Transit Database and cranked out quite a few interesting insights on United States Transit systems.\n1) The 2022 Fare Revenue table\n2) The 2022 Operating Expenses reports\n3) The latest Monthly Ridership tables\n\nsample_n(USAGE, 1000) |&gt; \n  mutate(month=as.character(month)) |&gt; \n  DT::datatable()\n\n\n\n\n\n1) MTA New York City Transit is the most traveled US transit system in terms of Vehicle Revenue Miles.\nAs a New Yorker, I’m convinced that New York City is the best city in the world. This statement however isn’t only supported by our top ranking in rats per household but also manifests itself in the number of people that take public transportation.\nAfter analyzing data for 678 different public transit agencies in the United States, and summing their vehicle revenue miles, we find that the NYC MTA takes the lead with 10.8B traveled miles. To put that into perspective, that’s the equivalent of three one-way trips to Pluto.\n\n#This code creates a the function \"Top Agency\" that allows us to group our data by transit agency and find out which has the highest total vehicle miles \ntopagency &lt;- USAGE |&gt;\n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(`Vehicle Revenue Miles`)) |&gt;\n  arrange(desc(total_vrm))\nhead(topagency,n=5)\n\n# A tibble: 5 × 2\n  Agency                                                     total_vrm\n  &lt;chr&gt;                                                          &lt;dbl&gt;\n1 MTA New York City Transit                                10832855350\n2 New Jersey Transit Corporation                            5645525525\n3 Los Angeles County Metropolitan Transportation Authority  4354016659\n4 Washington Metropolitan Area Transit Authority            2821950701\n5 Chicago Transit Authority                                 2806202144\n\n\nIf that stat didn’t sell you on the MTA’s public transit dominance, if we look at the next two most utilized public transportation systems in the US, they don’t even come close in terms of miles traveled. (NJT coming in at 5.6B and LACTA coming in at 4.3B)\nTo be fair, these numbers make sense if we take a look at how many trips were taken on the NYC Subway (Heavy rail) in May of 2024 by itself. By filtering our data for NYC MTA, Heavy Rail, and 2024-05-01, we can see the following number of trips taken…\n\n#This code changes our month column to characters so we can use a filter on it\nUSAGE$Month &lt;- as.character(USAGE$month)\n\n#In order to find out the specific number of riders, we filtered our data for NYC MTA, Heavy rail , and the date.\nNYC_HR_MAY2024 &lt;- USAGE |&gt;\n  filter(USAGE$Agency == \"MTA New York City Transit\", USAGE$Mode==\"Heavy Rail\", USAGE$Month==\"2024-05-01\")\n  print(NYC_HR_MAY2024$`Unlinked Passenger Trips`)\n\n[1] 180458819\n\n\nExtrapolating and coverting this to a yearly amount result in ~2,165,505,828 trips which is a heck of a lot. In fact, this number doesn’t even account for the impact COVID-19 had on subway ridership.\nCOVID-19 and its Impact on NYC MTA Ridership\nTo analyze the impact that COVID-19 had on the MTA, we can compare NYC MTA unlinked passenger trips pre-pandemic and MTA unlinked passenger trips during the pandemic. For this example, I pulled data for the NYC MTA heavy rail for April 2019 and April 2020 and this is how it came out.\n\n#To measure the impact of covid we can compare pre-covid to during the pandemic by filtering for our data according to our specific requirements like NYC MTA & Heavy rail\n\nNYC_HR_APRIL2019 &lt;- USAGE |&gt;\n  filter(USAGE$Agency==\"MTA New York City Transit\", USAGE$Mode==\"Heavy Rail\", USAGE$Month==\"2019-04-01\")\n\nNYC_HR_APRIL2020 &lt;- USAGE |&gt;\n  filter(USAGE$Agency==\"MTA New York City Transit\", USAGE$Mode==\"Heavy Rail\", USAGE$Month==\"2020-04-01\")\n\nprint(NYC_HR_APRIL2019$`Unlinked Passenger Trips`)- print(NYC_HR_APRIL2020$`Unlinked Passenger Trips`)\n\n[1] 232223929\n[1] 20254269\n\n\n[1] 211969660\n\n\nThe first number represents pre-pandemic activity of 232M unlinked passenger trips in April of 2019 which dropped significantly to 2M in April 2020 representing 99% drop! If we were to visualize that drop it would look something like this…\n\n#This code helps us visualize the impact of Covid by summing unlinked passenger trips in relation to the time periods they were recorded.\nNYC_HR_Seasonality &lt;- USAGE |&gt;\n  group_by(month)|&gt;\n  filter(Agency==\"MTA New York City Transit\", Mode==\"Heavy Rail\")|&gt;\n  summarize(total_UPT = sum(`Unlinked Passenger Trips`)) |&gt;\n  arrange(desc(total_UPT))\n\nplot(NYC_HR_Seasonality,main=\"New York City Ridership from 2002-2023\", xlab=\"Time\", ylab=\"Ridership\",ylim = c(0, 400000000))\n\n\n\n\n\n\n\n\nAs you can see, MTA Transit ridership has steadily grown from 2002-2019 until the pandemic took place. Since then, MTA ridership has not fully recovered. If we were to take this into account and use pre-pandemic numbers as an estimate of yearly ridership, we could have seen ridership volumes of up to 2.75B!\nAlthough I could talk about New York City all day, I’m sure you’d find it more interesting if I threw some other discoveries at you.\n2) The United States Loves Buses\nWith all the talk about heavy rails and subways, you would figure that they’d be the most popular form of public transportation in the United States… Well, you guessed wrong.\nAfter pulling ridership data, grouping it by transportation mode, and summing the total vehicle miles… Buses came out on top with 49,444,494,088 total miles traveled! (Approximately 13.75 Pluto trips!)\n\n#Our code here sorts by mode of transportation and then sums up the total VRM to gauge which mode traveled the most\ntopmode &lt;- USAGE |&gt; \n  group_by(Mode) |&gt; \n  summarize(total_vrm = sum(`Vehicle Revenue Miles`)) |&gt; \n  arrange(desc(total_vrm)) \nhead(topmode,n=1)\n\n# A tibble: 1 × 2\n  Mode    total_vrm\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n\nIf we parse a little deeper we can see that the top and bottom contributors for this stat were..\n\n#This will display the top bus in terms of vehicle revenue miles\ntoponebus &lt;- USAGE |&gt;\n  group_by(Agency)|&gt;\n  filter(Mode==\"Bus\")|&gt;\n  summarize(total_VRM = sum(`Vehicle Revenue Miles`)) |&gt;\n  arrange(desc(total_VRM))\nhead(toponebus,n =1)\n\n# A tibble: 1 × 2\n  Agency                          total_VRM\n  &lt;chr&gt;                               &lt;dbl&gt;\n1 New Jersey Transit Corporation 3781858802\n\n\n\n#This will display the bottom bus in terms of vehicle revenue miles\n\nbottomonebus &lt;- USAGE |&gt;\n  group_by(Agency)|&gt;\n  filter(Mode==\"Bus\")|&gt;\n  summarize(total_VRM = sum(`Vehicle Revenue Miles`)) |&gt;\n  arrange(desc(total_VRM))\ntail(bottomonebus,n =1)\n\n# A tibble: 1 × 2\n  Agency                          total_VRM\n  &lt;chr&gt;                               &lt;dbl&gt;\n1 Windham Region Transit District     21265\n\n\nWell, what do you know! The NJT Corporation lead with a staggering total of 3,781,858,802 vehicle revenue miles while the Windham Region Transit dragged down the average with a measly 21,265.\n3) The Award for least used Public Transportation goes to the Municipality of Carolina Demand Response\nFortunately for the Winham transit, the Municipality of Carolina demand response takes the award for the least used public transportation. Contrary to its name, this transit system does not serve communities in North Carolina or South Carolina but instead operates within the municipality of Carolina, Puerto Rico. After pulling and sorting the data for the municipality in R, I found that this transit system served a total whopping 225 unlinked passenger trips!\n\n#This will display the least popular mode of tranpsortation in terms of UPT\n\nleastusedupt &lt;- USAGE |&gt;\n  group_by(Agency,Mode)|&gt;\n  summarize(total_UPT = sum(`Unlinked Passenger Trips`)) |&gt;\n  arrange(desc(total_UPT))\ntail(leastusedupt,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                   Mode            total_UPT\n  &lt;chr&gt;                    &lt;chr&gt;               &lt;dbl&gt;\n1 Municipality of Carolina Demand Response       225\n\n\n4) The United States Public Transportation System faces a Significant Funding Gap.\nWhile the United States public transit systems provide a vital service for millions of commuters, operating costs often exceed revenues. Farebox revenue, typically the largest source of income for transit systems, often falls short of covering expenses which leads to unprofitable modes of public transportation.\nFor example, the New York City MTA had the most unlinked passenger trips at an astounding 1,793,073,801 in 2022 but it’s farebox recovery ratio was rather lackluster 0.325. This means that the NYC MTA is an extremely unprofitable transit system and for every dollar the NYC MTA spends, it only brings in $0.325 of revenue.\n\n#This will display the top transit system in terms of UPT for the year 2022 by grouping agency and mode & summing total_upt. Additional sorts out for larger transit systems via total_UPT&gt;400000\n\ntopUPT2022 &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(total_UPT2022 = sum(total_UPT)) |&gt;\n  arrange(desc(total_UPT2022))\nhead(topUPT2022,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode       total_UPT2022\n  &lt;chr&gt;                     &lt;chr&gt;              &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail    1793073801\n\n\n\n#This will display farebox recovery for NYC by dividing total fares by expenses.\nnycmtafarebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  filter(Agency == \"MTA New York City Transit\")|&gt;\n  summarize(nycfarebox = sum(`Total Fares`)/sum(Expenses)) |&gt;\n  arrange(desc(nycfarebox))\nhead(nycmtafarebox,n =1)\n\n# A tibble: 1 × 1\n  nycfarebox\n       &lt;dbl&gt;\n1      0.325\n\n\nAs a result of low farebox revenues, transit agencies rely heavily on government subsidies, including federal, state, and local funds. While these subsidies are essential for maintaining operations, it is interesting to take a look at which transit systems are the best self-sustaining and most efficient with their resources. This, however, is not an easy task as there are many metrics to measure efficiency.\nFor example:\nThe most profitable transit system in terms of profits/expenses is the Port Imperial Ferry Corporation which serves the NY/NJ region with a farebox recovery ratio of 1.423.\n\n#This will display the top agency in terms of farebox recovery \ntopfarebox &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(topfarebox = sum(`Total Fares`)/sum(Expenses)) |&gt;\n  arrange(desc(topfarebox))\nhead(topfarebox,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                          Mode      topfarebox\n  &lt;chr&gt;                           &lt;chr&gt;          &lt;dbl&gt;\n1 Port Imperial Ferry Corporation Ferryboat       1.43\n\n\nThe transit system with the lowest expenses per unlinked passenger trip is North Carolina’s State University Bus which spends an average $1.18 USD spent per trip.\n\n#This will display the agency with lowest expeneses per upt\nlowexpupt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(lowexp = sum(Expenses)/sum(total_UPT)) |&gt;\n  arrange(desc(lowexp))\ntail(lowexpupt,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                          Mode  lowexp\n  &lt;chr&gt;                           &lt;chr&gt;  &lt;dbl&gt;\n1 North Carolina State University Bus     1.18\n\n\nThe transit system with the highest revenues per UPT is the Hampton Jitney Incorporated Commuter Bus with a solid $41.3 per trip.\n\n#This will display the agency with highest rev per UPT\nhighexpupt &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(highexp = sum(`Total Fares`)/sum(total_UPT)) |&gt;\n  arrange(desc(highexp))\nhead(highexpupt,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency               Mode         highexp\n  &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n1 Hampton Jitney, Inc. Commuter Bus    41.3\n\n\nThe system with the lowest expenses per traveled mile is the Metropolitan Transportation Commission Vanpool with a total $0.44 spent per every mile traveled.\n\n#This will display the agency with lowest expense per VRM\nlowexpvrm &lt;- USAGE_AND_FINANCIALS|&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(lvrm = sum(Expenses)/sum(total_VRM))|&gt;\n  arrange(desc(lvrm))\ntail(lowexpvrm, n=1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                                 Mode     lvrm\n  &lt;chr&gt;                                  &lt;chr&gt;   &lt;dbl&gt;\n1 Metropolitan Transportation Commission Vanpool 0.445\n\n\nThe transit system with the highest total fares per VRM is the Jacksonville Transportation Authority ferry boat with a towering $157.7 per VRM.\n\n#This will display the agency with highest fares per VRM\nhighfarevrm &lt;- USAGE_AND_FINANCIALS |&gt;\n  group_by(Agency,Mode)|&gt;\n  filter(total_UPT&gt;400000)|&gt;\n  summarize(highfarepvrm = sum(`Total Fares`)/sum(total_VRM)) |&gt;\n  arrange(desc(highfarepvrm))\nhead(highfarevrm,n =1)\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                                Mode      highfarepvrm\n  &lt;chr&gt;                                 &lt;chr&gt;            &lt;dbl&gt;\n1 Jacksonville Transportation Authority Ferryboat         158.\n\n\n\nConclusion: Port Imperial is the most efficient in terms of profitability and the NYC MTA is the most efficient in terms of sheer transportation\nWhile efficiency is hard to measure with one metric, by far the Port Imperial Ferry Corporation blows the competition out of the water in terms of pure profitability. The agency is so profitable that it has been able to fund itself without government subsidies while serving 7m+ unlinked trips annually.\nDespite Port Imperial leading the group in profitability, it serves a much smaller range of constituents than its next-door neighbor,the MTA. The NYC MTA has the capability to help commuters make over 1.8B trips annually with the capacity to grow even more. In terms of sheer transportation, it is impossible to beat and deserves a spot as one of the most efficient transit systems due the scale that is has been able to obtain."
  },
  {
    "objectID": "MP02.html",
    "href": "MP02.html",
    "title": "Creating the Statistically Correct Movie",
    "section": "",
    "text": "Introduction:\nIn a world inundated with cinematic mediocrity, we’ve become accustomed to the dreaded sequel. A once-promising franchise, often reduced to a shadow of its former self, plagued by plot holes, uninspired performances, and a seemingly endless pursuit of box office dollars. This is a tragedy that has befallen my favorite franchise, the Star Wars Universe.\nWhat was once a beautifully constructed story was turned upside down in the hands of Disney when they released the subpar Force Awakens trilogy. Although it has been nine years since then, it still comes to haunt me every May 4th.\nTo help myself cope with my Star Wars-induced depression, I wanted to create a model to measure the success of the franchise’s old films compared to some of the long-time greats and see if they deserved a spot in the hall of fame.\n\n\nLoading our Packages:\nNow before we start, it’s time to set up our environment with a few packages to help us manipulate our data later.\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(tidyr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(plotly)\n\nFor our ultimate metric, we have to look through history to see what movies and shows performed the best. To do this, I used the following code to pull a smaller subsection of data from IMDB (so that my computer won’t melt down) with the following code.\n\n\nLoading Data:\n\n#This code is a workaround way to load in our datasets\nNAME_BASICS &lt;- suppressWarnings(readr::read_csv(\"name_basics_small.csv.zip\"))\nTITLE_BASICS &lt;- suppressWarnings(readr::read_csv(\"title_basics_small.csv.zip\"))\nTITLE_CREW &lt;- suppressWarnings(readr::read_csv(\"title_crew_small.csv.zip\"))\nTITLE_EPISODES &lt;- suppressWarnings(readr::read_csv(\"title_episodes_small.csv.zip\"))\nTITLE_PRINCIPALS &lt;- suppressWarnings(readr::read_csv(\"title_principals_small.csv.zip\"))\nTITLE_RATINGS &lt;- suppressWarnings(readr::read_csv(\"title_ratings_small.csv.zip\"))\n\n\n\nCode\n#Issue with code loading data from prof website\n#get_imdb_file &lt;- function(fname){\n#    BASE_URL &lt;- \"https://github.com/michaelweylandt/STA9750/tree/main/miniprojects/mini02_preprocessed/\"\n#    fname_ext &lt;- paste0(fname, \".csv.zip\")\n#    if(!file.exists(fname_ext)){\n#        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n#        download.file(FILE_URL, \n#                      destfile = fname_ext)\n#    }\n#    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n#}\n\n#NAME_BASICS      &lt;- get_imdb_file(\"name_basics_small\")\n#TITLE_BASICS     &lt;- get_imdb_file(\"title_basics_small\")\n#TITLE_EPISODES   &lt;- get_imdb_file(\"title_episodes_small\")\n#TITLE_RATINGS    &lt;- get_imdb_file(\"title_ratings_small\")\n#TITLE_CREW       &lt;- get_imdb_file(\"title_crew_small\")\n#TITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\n\n\n\n\nTrimming and Correcting Data Sets\nHm… Although we’ve already selected a filtered subset of IMDB data, our NAME_BASICS data set still seems too large and is slowing our program down. To fix this, we can whittle it down even further by removing people who are known for less than two productions in our NAME_BASICS dataset.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\nSweet! Now that our sets are a little smaller, let’s take a look at the distribution of ratings for the media we have left over.\n\n\nCode\n#code to create plot\nTITLE_RATINGS |&gt;\n  ggplot(aes(x=numVotes)) + \n  geom_histogram(bins=30) +\n  xlab(\"Number of IMDB Ratings\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"IMDB Titles and Rating Distributions\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\n\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n     0%     25%     50%     75%    100% \n    100     165     332     970 2942823 \n\n\nIt seems like our data is heavily skewed right although our pre-filtered data set has already removed many of the movies/shows with under 100 ratings. Just in case, let’s run our filter again to be sure.\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt;= 100)\n\n\n\nCode\n#Code to join & create new tables\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nNow that we’ve cut down our data set to a more manageable size, it’s important to make sure that all our column types are correct so we can work with our data. We can take a look at our data sets with the glimpse function.\n\nglimpse(TITLE_BASICS)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, 1…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nAs we can see, a few of our column types aren’t correct. For example, we want ‘start year’, ‘end year’, and ‘run time minutes’ to be numeric values and our ‘adult rating’ to be a logical value. To fix this, we can mutate the Title Basics data set to change those values…\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes),\n    isAdult = as.logical(isAdult)\n)\n\n(I also took the liberty to correct the rest of our sets if you’d like to take a look.)\n\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…\n\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\n# A tibble: 10 × 6\n   nconst    primaryName    birthYear deathYear primaryProfession knownForTitles\n   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;         \n 1 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0072308     \n 2 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0050419     \n 3 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0053137     \n 4 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0027125     \n 5 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0037382     \n 6 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0075213     \n 7 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0117057     \n 8 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0038355     \n 9 nm0000003 Brigitte Bard…      1934        NA actress,music_de… tt0057345     \n10 nm0000003 Brigitte Bard…      1934        NA actress,music_de… tt0049189     \n\n\nNow that our data is ready to be manipulated, we can try some exercises on it.\n\n\nExercises:\n2.0 : How many movies are in our dataset? (For this case we’ll just use movies and not TVmovies)\n\n#We can sort our TITLE_BASICS dataset by filtering title type to movie and counting how many rows exist\ntotalmovies &lt;- TITLE_BASICS |&gt;\n  filter(TITLE_BASICS$titleType==\"movie\")\n  nrow(totalmovies)\n\n[1] 131662\n\n\nHow many TV Series?\n\n#We can sort our TITLE_BASICS dataset by filtering title type to TVseries and counting how many rows exist\ntotal_TV_Series &lt;- TITLE_BASICS |&gt;\n  filter(TITLE_BASICS$titleType==\"tvSeries\")\n  nrow(total_TV_Series)\n\n[1] 29789\n\n\nHow many episodes?\n\n#We can sort our TITLE_BASICS dataset by filtering title type to episodes and counting how many rows exist\ntotal_TV_Episodes &lt;- TITLE_BASICS |&gt; \n  filter(TITLE_BASICS$titleType==\"tvEpisode\") \n  nrow(total_TV_Episodes)\n\n[1] 155722\n\n\nSweet! It seems like we have way more movies than TV series which makes sense. What about the rest of the media types?\n\n\nCode\n#This code counts the quantity of each title type\n\ntitle_counts &lt;- TITLE_BASICS |&gt;\n    count(titleType)\n\n#This code creates a bar chart using Media type as an x axis and quantity in our dataset as a y axis\n\nggplot(title_counts, aes(x = titleType, y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Quantity of Media Types\", x = \"Title Type\", y = \"Quantity\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWell to no surprise, it seems as though movies and TV series dwarf the rest.\n2.2: How about if we try to find out who the oldest production crew member is that is still alive?\n\n\nCode\n#We can filter out NA death years meaning the the actor is still alive, NA birthyears to filter out missing data, and for birthyears greater than 1908 which is the oldest person alive right now! \noldestcrew &lt;- NAME_BASICS |&gt;\n  filter(is.na(NAME_BASICS$deathYear), !is.na(NAME_BASICS$birthYear), birthYear &gt;= 1934) |&gt;\n  arrange(desc(birthYear)) |&gt;\n  tail(1)  \n\nprint(oldestcrew)\n\n\n# A tibble: 1 × 6\n  nconst    primaryName   birthYear deathYear primaryProfession knownForTitles  \n  &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n1 nm9631985 Kurt Breucker      1934        NA \"\\\\N\"             tt5445184,tt102…\n\n\nWow! Kurt Breucker is still hanging in there at 90 years old!\n2.3: How about finding the perfect 10/10 rating show with over 200,000 votes?\n\n#To find our perfect episode we are filtering for episodes that have an average rating of 10 and have over 200,000 ratings. \nperfect_episode &lt;- TITLE_RATINGS |&gt;\n  filter(TITLE_RATINGS$averageRating == 10, TITLE_RATINGS$numVotes &gt;= 200000) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt; #Joins our ratings and basics datasets\n  head(1) \nprint(perfect_episode$primaryTitle)\n\n[1] \"Ozymandias\"\n\n\nTo be honest I have zero clue what this show is but perhaps I might give it a watch after discovering its rating!\n2.4: How about Mark Hamill’s top four projects? (If you don’t know… he plays Luke Skywalker in the Star wars original and sequel trilogies!)\n\n\nCode\n#We can create a variable for solely Mark Hamills projects\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt; #seperates Knownfortitles by ,\n  select(knownForTitles)\n\n#Join together our Mark Hamill set with Title_Basics to get titles\ntop4markhamill &lt;- mark_hamill |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle)\n\n#Create a datatable from our joint datasets\ndatatable(top4markhamill,\n          colnames = c(\"Project Name\"),\n          caption = \"Mark Hamill's Top Projects\"\n)\n\n\n\n\n\n\n2.5: Or what TV series with more than 12 episodes has the highest average rating?\n\n\nCode\n#Code to filter for tvSeries with more than 12 episodes\ntvseries12episodes &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;  \n  left_join(TITLE_EPISODES, \n            join_by(tconst == parentTconst)) |&gt;  \n  group_by(tconst, primaryTitle) |&gt;  \n  summarise(total_episodes = n()) |&gt; \n  filter(total_episodes &gt; 12)\n\n#code to join episodes with ratings\nbesttvseries12episodes &lt;- tvseries12episodes |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt; \n  ungroup() |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select( primaryTitle, averageRating) |&gt;\n  slice_head(n=10)\nhead(besttvseries12episodes,1)\n\n\n# A tibble: 1 × 2\n  primaryTitle averageRating\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Craft Games            9.7\n\n\n2.6: And lastly… is it true that episodes from later seasons of Happy Days have lower average ratings than the earlier seasons after their “Jump the shark” scene?\n\n\nCode\n#We filter out title basics for happy days to isolate and use tconst in join\njumptheshark &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\")\n#This code joins our title episodes and ratings\njumpthesharkimpact &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst %in% jumptheshark$tconst) |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(avg_rating = mean(averageRating)) |&gt;\n  arrange(seasonNumber)\n#This code creates a visualization of average rating over seasons\nggplot(jumpthesharkimpact, aes(x = seasonNumber, y = avg_rating)) +\n  geom_line(color = \"black\") + \n  geom_point(color = \"black\") +\n  labs(\n    title = \"Ratings of Happy Days by Season\",\n    x = \"Season Number\",\n    y = \"Average IMDB Rating\"\n  ) \n\n\n\n\n\n\n\n\n\nAccording to our visualization, it seems like ratings plummeted throughout the next two seasons after the “Jump the shark” episode but recovered from seasons 8-11. Despite recovering in later seasons, the earlier seasons of the show on average have higher ratings.\n\n\nCreating a Success Metric:\nAs I looked through the IMBD data sets, it made sense that the TITLE_RATINGS data set would contain the best measurable variables like average rating and number of votes.\nBased upon this, I decided to create the following success metric:\n\nSuccess Metric = (Average rating * log(Number of Votes))\n\nThis metric is a weighted score that gives more weight to movies with both high ratings and a large number of votes while also reducing the heavy right skew of our data set using a logorithmic function.\nTesting the Metric\n3.1: Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\n\n\nCode\n#This code joins together Ratings and Title Basics to create a dataset with names and success scores\nmoviesuccess &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(success_score = averageRating * log10(numVotes)) |&gt;\n  arrange(desc(success_score))\n\nhead(moviesuccess %&gt;% select(primaryTitle,averageRating,numVotes, success_score), 5)\n\n\n# A tibble: 5 × 4\n  primaryTitle                              averageRating numVotes success_score\n  &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 The Shawshank Redemption                            9.3  2942823          60.2\n2 The Dark Knight                                     9    2922922          58.2\n3 The Godfather                                       9.2  2051186          58.1\n4 The Lord of the Rings: The Return of the…           9    2013824          56.7\n5 Pulp Fiction                                        8.9  2260017          56.6\n\n\nBased upon this test, I would say that our weighted success metric proves effective as all of these movies fall within the top 13 movies of all time on IMDB and were box office successes.\n3.2: Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\n\n\nCode\n#We can filter movies with 100k+ votes and arrange based on success score\nmoviesuccess |&gt;\n  filter(numVotes &gt; 100000) |&gt;  \n  arrange(success_score) |&gt;  \n  head(5) |&gt;  \n  select(primaryTitle, averageRating, numVotes, success_score)  \n\n\n# A tibble: 5 × 4\n  primaryTitle      averageRating numVotes success_score\n  &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Radhe                       1.9   180205          9.99\n2 Epic Movie                  2.4   110222         12.1 \n3 Adipurush                   2.7   133981         13.8 \n4 Meet the Spartans           2.8   112199         14.1 \n5 365 Days                    3.3   100579         16.5 \n\n\nBased upon this test, I would also say that our weighted success metric proves effective in weighing movies that have bombed based upon large numbers of IMDb votes.\n3.3: Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\n\n\nCode\n#This code takes our Hamill data set and joins it with movie success \nmark_hamill_movies &lt;- mark_hamill |&gt;\n  left_join(moviesuccess, by = c(\"knownForTitles\" = \"tconst\"))\n\n#This code arranges our new dataset by success score\nmark_hamill_movies |&gt;\n  arrange(desc(success_score)) |&gt;\n  head(5)|&gt;\n  select(primaryTitle, averageRating, numVotes, success_score)\n\n\n# A tibble: 4 × 4\n  primaryTitle                              averageRating numVotes success_score\n  &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Star Wars: Episode V - The Empire Strike…           8.7  1401591          53.5\n2 Star Wars: Episode IV - A New Hope                  8.6  1471222          53.0\n3 Star Wars: Episode VI - Return of the Je…           8.3  1137692          50.3\n4 Star Wars: Episode VIII - The Last Jedi             6.9   681588          40.3\n\n\nUsing Mark Hamill’s data that we already prepped before, we can join it with our movie success data set to see that his four movies all rank rather high on our metric. (As they should being great Starwars movies!)\n3.4: Perform at least one other form of ‘spot check’ validation.\nWhat better way to do a spot-check validation than to use our metric against one of the biggest sequel flops of all time, Grease 2.\n\n\nCode\n#This code filters out for Grease 2 in primary title\nmoviesuccess |&gt;\n  filter(primaryTitle == \"Grease 2\") |&gt;\n  head(1) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score)\n\n\n# A tibble: 1 × 4\n  primaryTitle averageRating numVotes success_score\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Grease 2               4.6    39079          21.1\n\n\nAs expected, our success score hit almost rock bottom indicating that it is effective at determining low scoring movies.\n3.5: Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\nSince Grease 2 was determined to be a flop by many standards with a success score of 21.13, using a 95% threshold at 29.82 would be reasonable to consider a project to be a success.\n\nquantile(moviesuccess$success_score, probs = seq(.9, 1, by = 0.025))\n\n     90%    92.5%      95%    97.5%     100% \n25.85849 27.53372 29.82612 33.47237 60.15951 \n\nsuccess_value &lt;- 29.82\n\n\n\nExamining Success by Genre and Decade\n4.1: What was the genre with the most “successes” in each decade?\nTo answer this question, we would first need to sort the movies into their respective genres and release dates of movies into decades.\n\n#Seperating movies with multiple genres into different lines\nmovie_genre &lt;- moviesuccess |&gt;\n  separate_rows(genres, sep = \",\")\n  \n#filtering out NA values from genres and start years\nmoviesuccess_separated &lt;- movie_genre |&gt;\n  filter(!is.na(genres) & !is.na(startYear)) |&gt; \n  mutate(decade = floor(startYear / 10) * 10) \n\nNext we can work on visualizing our new data set to get a clear view of movie preferences over the decades.\n\n\nCode\n#This code filters out succesful movies and groups them by decade then genre\n decade_success &lt;- moviesuccess_separated |&gt;\n  filter(success_score &gt;= success_value) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    quantity = n(), \n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(quantity, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = dense_rank(desc(quantity))) |&gt;\n  ungroup() |&gt;\n  arrange(decade, rank) \n\nlibrary(randomcoloR)\nn &lt;- 20\npalette &lt;- distinctColorPalette(n)\n\n\n#This code creates a graph visualization of our data set\nggplot(decade_success, aes(x = decade, y = quantity, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Successful Movies by Decade and Genre\", x = \"Decade\", y = \"Quantity\", fill = \"Genre\") +\n  scale_fill_manual(values = distinctColorPalette(n)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n4.2: What genre consistently has the most “successes”? What genre used to reliably produce “successes” and has fallen out of favor?\nAccording to our chart, we can see that the Drama category has consistently outperformed the rest of the genres over the decades while romance seems to have fallen out for favor going from 5th place in the 2000’s to nearly last in the 2020’s.\n4.3: What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nDrama has produced the most successes since 2010 and it does seem to have a large number of successes due to production size rather than a higher success rate. In fact, dramas only have a 6% success rate compared to less popular biographies with a whopping 13%!\n\n\nCode\nmovie_genre |&gt;\n  group_by(genres) |&gt;\n  summarize(\n    Total_Movies = n(),\n    Successes = sum(success_score &gt;= success_value),\n    Success_Rate_In_Percent = round(Successes / Total_Movies * 100),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(Success_Rate_In_Percent)) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n4.4: What genre has become more popular in recent years?\nIt seems as though action has been gaining traction in recent years jumping from ~4th most popular in the 90’s to ~2nd most popular in the 2020’s.\n\n\nSuccessful Personnel in the Genre\nBased on my analysis, if I were to make a successful movie I would create a biography as they historically have the highest probability of being successful. As a result, I would look to cast an actor and director who specialize in the genre.\n\n\nCode\n#Code to filter for movies that fall into the biography genre\nmoviesuccess_biography &lt;- moviesuccess_separated |&gt;\n  filter(genres == \"Biography\")\n  \n#Code to join actor names to the movies they are in and sum their success scores\nsuccessful_biography_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(moviesuccess_biography, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_score &gt;= success_value),\n    avg_success_score = round(mean(success_score, na.rm = TRUE),2),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n#code to join names to movies\nsuccessful_biography_actors &lt;- successful_biography_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_score)\n\nsuccessful_biography_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\nBased on my analysis of our movie data for the top biography actors, I’ve decided to select Christian Bale and Matthew McConaughey as my actors. Their ability to portray complex, morally ambiguous characters like Patrick Bateman and Mark Hanna demonstrates their suitability for a darker, more twisted biography. Given their history of success in the genre and their ability to convey nuanced emotions, I believe that they would be invaluable assets to a biographical film exploring morally complex figures in a war setting. Along with that, both of these actors fall within the top nine most successful biographical movies as well as the top four in average success score.\n\n\nCode\n#Code to filter for movies that fall into the biography genre\nmoviesuccess_biography &lt;- moviesuccess_separated |&gt;\n  filter(genres == \"Biography\")\n\n  \n#Code to join director names to the movies they are in and sum their success scores\nsuccessful_biography_directors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(moviesuccess_biography, by = \"tconst\") |&gt;\n  filter(category == \"director\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_score &gt;= success_value),\n    avg_success_score = mean(success_score, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n#code to join names to movies\nsuccessful_biography_directors &lt;- successful_biography_directors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_score)\n\nsuccessful_biography_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\nIn terms of a director, there is no other clear choice to me than Steven Speilberg. Despite falling lower on the list in terms of successful biographical movies, Spielberg represents the cream of the crop in terms of average success score. Steven Spielberg’s masterful storytelling and ability to draw exceptional performances from actors would undoubtedly complement the strengths of Matthew McConaughey and Christian Bale. Spielberg’s visionary direction and keen eye for detail in works like saving Private Ryan would provide a rich, dark, and immersive world for both these talented actors to inhabit. His ability to elicit powerful emotional performances would allow McConaughey and Bale to fully explore the complexities of their characters, bringing their unique talents to the forefront.\n\n\nFinding a Classic to Remake:\nThe movie I want to remake it Schindler’s List from 1993. This movie is a historical drama film that tells the true story of Oskar Schindler,a German businessman who saved the lives of hundreds of Jews during the Holocost. Schindlers List has a whopping 9 rating on IMDB with over 1.4mm votes giving a high success score of 55.52 based on our metric. Along with that, this movie was produced over 25 years ago and having Steven Spielberg on board would make it easy to obtain the rights to reproduce it. (As he was the original director of the movie) On the other hand, our legal team would have to reach out to the former key actors like Liam Neeson and Ralph Fiennes regarding rights to the project.\n\nmoviesuccess_biography |&gt;\n  select(primaryTitle, startYear, success_score, numVotes, averageRating) |&gt;\n  DT::datatable(options = list(pageLength = 5, order = list(list(3, \"desc\"))))\n\n\n\n\n\n\n\nWrite and Deliever your Pitch\nDramas have reigned supreme in the film industry for decades and has consistently been the best performing genre for the last 100 years. With over 3400 titles created between 1990 onwards it is no surprise just how effective these movies are at evoking powerful emotions and creating heartfelt stories.\nSteven Spielberg, a visionary director with a proven track record of success, has a knack for crafting thought-provoking dramas that make people wrap heads around difficult to solve moral dilemmas. His collaboration with Matthew McConaughey and Christian Bale, two of Hollywood’s most acclaimed actors known for playing morally ambiguous roles, would create a powerful and unforgettable cinematic experience.\nSchindler’s List, a timeless tale of courage, compassion, and the human spirit, is ripe for a modern reimagining. With Spielberg’s masterful direction and the exceptional talents of McConaughey and Bale, this film promises to be a powerful exploration of relevant themes.\nThis new adaptation will captivate audiences with its compelling narrative, unforgettable performances, and stunning visuals. It’s a must-see film that will leave a lasting impression."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Creating the Statistically Correct Movie",
    "section": "",
    "text": "Introduction:\nIn a world inundated with cinematic mediocrity, we’ve become accustomed to the dreaded sequel. A once-promising franchise, often reduced to a shadow of its former self, plagued by plot holes, uninspired performances, and a seemingly endless pursuit of box office dollars. This is a tragedy that has befallen my favorite franchise, the Star Wars Universe.\nWhat was once a beautifully constructed story was turned upside down in the hands of Disney when they released the sub-par Force Awakens trilogy. Although it has been nine years since then, it still haunts me every May 4th.\nTo help myself cope with my Star Wars-induced depression, I wanted to create a model to measure the success of the franchise’s old films compared to some of the long-time greats and see if they deserved a spot in the hall of fame.\n\n\nLoading our Packages:\nBefore we start, it’s time to set up our environment with a few packages to help us manipulate our data later.\n\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(tidyr)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(plotly)\n\nFor our ultimate metric, we have to look through history to see what movies and shows performed the best. To do this, I used the following code to pull a smaller subsection of data from IMDB (so that my computer won’t melt down) with the following code.\n\n\nLoading Data:\n\n#This code is a workaround way to load in our datasets\nNAME_BASICS &lt;- suppressWarnings(readr::read_csv(\"name_basics_small.csv.zip\"))\nTITLE_BASICS &lt;- suppressWarnings(readr::read_csv(\"title_basics_small.csv.zip\"))\nTITLE_CREW &lt;- suppressWarnings(readr::read_csv(\"title_crew_small.csv.zip\"))\nTITLE_EPISODES &lt;- suppressWarnings(readr::read_csv(\"title_episodes_small.csv.zip\"))\nTITLE_PRINCIPALS &lt;- suppressWarnings(readr::read_csv(\"title_principals_small.csv.zip\"))\nTITLE_RATINGS &lt;- suppressWarnings(readr::read_csv(\"title_ratings_small.csv.zip\"))\n\n\n\nCode\n#Issue with code loading data from prof website\n#get_imdb_file &lt;- function(fname){\n#    BASE_URL &lt;- \"https://github.com/michaelweylandt/STA9750/tree/main/miniprojects/mini02_preprocessed/\"\n#    fname_ext &lt;- paste0(fname, \".csv.zip\")\n#    if(!file.exists(fname_ext)){\n#        FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n#        download.file(FILE_URL, \n#                      destfile = fname_ext)\n#    }\n#    as.data.frame(readr::read_tsv(fname_ext, lazy=FALSE))\n#}\n\n#NAME_BASICS      &lt;- get_imdb_file(\"name_basics_small\")\n#TITLE_BASICS     &lt;- get_imdb_file(\"title_basics_small\")\n#TITLE_EPISODES   &lt;- get_imdb_file(\"title_episodes_small\")\n#TITLE_RATINGS    &lt;- get_imdb_file(\"title_ratings_small\")\n#TITLE_CREW       &lt;- get_imdb_file(\"title_crew_small\")\n#TITLE_PRINCIPALS &lt;- get_imdb_file(\"title_principals_small\")\n\n\n\n\n\nTrimming and Correcting Data Sets\nHm… Although we’ve already selected a filtered subset of IMDB data, our NAME_BASICS data set still seems too large and is slowing our program down. To fix this, we can whittle it down even further by removing people who are known for less than two productions in our NAME_BASICS dataset.\n\nNAME_BASICS &lt;- NAME_BASICS |&gt; \n  filter(str_count(knownForTitles, \",\") &gt; 1)\n\nSweet! Now that our sets are a little smaller, let’s take a look at the distribution of ratings for the media we have left over.\n\n\nCode\n#code to create plot\nTITLE_RATINGS |&gt;\n  ggplot(aes(x=numVotes)) + \n  geom_histogram(bins=30) +\n  xlab(\"Number of IMDB Ratings\") + \n  ylab(\"Number of Titles\") + \n  ggtitle(\"IMDB Titles and Rating Distributions\") + \n  theme_bw() + \n  scale_x_log10(label=scales::comma) + \n  scale_y_continuous(label=scales::comma)\n\n\n\n\n\n\n\n\n\n\nTITLE_RATINGS |&gt;\n  pull(numVotes) |&gt;\n  quantile()\n\n     0%     25%     50%     75%    100% \n    100     165     332     970 2942823 \n\n\nIt seems like our data is heavily skewed right although our pre-filtered data set has already removed many of the movies/shows with under 100 ratings. Just in case, let’s run our filter again to be sure.\n\nTITLE_RATINGS &lt;- TITLE_RATINGS |&gt;\n  filter(numVotes &gt;= 100)\n\n\n\nCode\n#Code to join & create new tables\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_CREW &lt;- TITLE_CREW |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\n\nTITLE_EPISODES_1 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(tconst == tconst))\nTITLE_EPISODES_2 &lt;- TITLE_EPISODES |&gt;\n  semi_join(TITLE_RATINGS, \n            join_by(parentTconst == tconst))\n\nTITLE_EPISODES &lt;- bind_rows(TITLE_EPISODES_1,\n                            TITLE_EPISODES_2) |&gt;\n  distinct()\n\nTITLE_PRINCIPALS &lt;- TITLE_PRINCIPALS |&gt;\n  semi_join(TITLE_RATINGS, join_by(tconst == tconst))\n\n\nrm(TITLE_EPISODES_1)\nrm(TITLE_EPISODES_2)\n\n\nNow that we’ve cut down our data set to a more manageable size, it’s important to make sure that all our column types are correct so we can work with our data. We can take a look at our data sets with the glimpse function.\n\nglimpse(TITLE_BASICS)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, 1…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nAs we can see, a few of our column types aren’t correct. For example, we want ‘start year’, ‘end year’, and ‘run time minutes’ to be numeric values and our ‘adult rating’ to be a logical value. To fix this, we can mutate the Title Basics data set to change those values…\n\nTITLE_BASICS &lt;- TITLE_BASICS |&gt;\n  mutate(\n    startYear = as.numeric(startYear),\n    endYear = as.numeric(endYear),\n    runtimeMinutes = as.numeric(runtimeMinutes),\n    isAdult = as.logical(isAdult)\n)\n\n(I also took the liberty to correct the rest of our sets if you’d like to take a look.)\n\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…\n\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\n# A tibble: 10 × 6\n   nconst    primaryName    birthYear deathYear primaryProfession knownForTitles\n   &lt;chr&gt;     &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;         \n 1 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0072308     \n 2 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0050419     \n 3 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0053137     \n 4 nm0000001 Fred Astaire        1899      1987 actor,miscellane… tt0027125     \n 5 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0037382     \n 6 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0075213     \n 7 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0117057     \n 8 nm0000002 Lauren Bacall       1924      2014 actress,soundtra… tt0038355     \n 9 nm0000003 Brigitte Bard…      1934        NA actress,music_de… tt0057345     \n10 nm0000003 Brigitte Bard…      1934        NA actress,music_de… tt0049189     \n\n\nNow that our data is ready to be manipulated, we can try some exercises on it.\n\n\nExercises:\n2.0 : How many movies are in our dataset? (For this case we’ll just use movies and not TVmovies)\n\n#We can sort our TITLE_BASICS dataset by filtering title type to movie and counting how many rows exist\ntotalmovies &lt;- TITLE_BASICS |&gt;\n  filter(TITLE_BASICS$titleType==\"movie\")\n  nrow(totalmovies)\n\n[1] 131662\n\n\nHow many TV Series?\n\n#We can sort our TITLE_BASICS dataset by filtering title type to TVseries and counting how many rows exist\ntotal_TV_Series &lt;- TITLE_BASICS |&gt;\n  filter(TITLE_BASICS$titleType==\"tvSeries\")\n  nrow(total_TV_Series)\n\n[1] 29789\n\n\nHow many episodes?\n\n#We can sort our TITLE_BASICS dataset by filtering title type to episodes and counting how many rows exist\ntotal_TV_Episodes &lt;- TITLE_BASICS |&gt; \n  filter(TITLE_BASICS$titleType==\"tvEpisode\") \n  nrow(total_TV_Episodes)\n\n[1] 155722\n\n\nSweet! It seems like we have way more movies than TV series which makes sense. What about the rest of the media types?\n\n\nCode\n#This code counts the quantity of each title type\n\ntitle_counts &lt;- TITLE_BASICS |&gt;\n    count(titleType)\n\n#This code creates a bar chart using Media type as an x axis and quantity in our dataset as a y axis\n\nggplot(title_counts, aes(x = titleType, y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Quantity of Media Types\", x = \"Title Type\", y = \"Quantity\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWell to no surprise, it seems as though movies and TV series dwarf the rest.\n2.2: How about if we try to find out who the oldest production crew member is that is still alive?\n\n\nCode\n#We can filter out NA death years meaning the the actor is still alive, NA birthyears to filter out missing data, and for birthyears greater than 1908 which is the oldest person alive right now! \noldestcrew &lt;- NAME_BASICS |&gt;\n  filter(is.na(NAME_BASICS$deathYear), !is.na(NAME_BASICS$birthYear), birthYear &gt;= 1934) |&gt;\n  arrange(desc(birthYear)) |&gt;\n  tail(1)  \n\nprint(oldestcrew)\n\n\n# A tibble: 1 × 6\n  nconst    primaryName   birthYear deathYear primaryProfession knownForTitles  \n  &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;           \n1 nm9631985 Kurt Breucker      1934        NA \"\\\\N\"             tt5445184,tt102…\n\n\nWow! Kurt Breucker is still hanging in there at 90 years old!\n2.3: How about finding the perfect 10/10 rating show with over 200,000 votes?\n\n#To find our perfect episode we are filtering for episodes that have an average rating of 10 and have over 200,000 ratings. \nperfect_episode &lt;- TITLE_RATINGS |&gt;\n  filter(TITLE_RATINGS$averageRating == 10, TITLE_RATINGS$numVotes &gt;= 200000) |&gt;\n  inner_join(TITLE_BASICS, by = \"tconst\") |&gt; #Joins our ratings and basics datasets\n  head(1) \nprint(perfect_episode$primaryTitle)\n\n[1] \"Ozymandias\"\n\n\nTo be honest I have zero clue what this show is but perhaps I might give it a watch after discovering its rating!\n2.4: How about Mark Hamill’s top four projects? (If you don’t know… he plays Luke Skywalker in the Star wars original and sequel trilogies!)\n\n\nCode\n#We can create a variable for solely Mark Hamills projects\nmark_hamill &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt; #seperates Knownfortitles by ,\n  select(knownForTitles)\n\n#Join together our Mark Hamill set with Title_Basics to get titles\ntop4markhamill &lt;- mark_hamill |&gt;\n  left_join(\n    TITLE_BASICS,\n    join_by(knownForTitles == tconst)\n  ) |&gt;\n  select(primaryTitle)\n\n#Create a datatable from our joint datasets\ndatatable(top4markhamill,\n          colnames = c(\"Project Name\"),\n          caption = \"Mark Hamill's Top Projects\"\n)\n\n\n\n\n\n\n2.5: Or what TV series with more than 12 episodes has the highest average rating?\n\n\nCode\n#Code to filter for tvSeries with more than 12 episodes\ntvseries12episodes &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;  \n  left_join(TITLE_EPISODES, \n            join_by(tconst == parentTconst)) |&gt;  \n  group_by(tconst, primaryTitle) |&gt;  \n  summarise(total_episodes = n()) |&gt; \n  filter(total_episodes &gt; 12)\n\n#code to join episodes with ratings\nbesttvseries12episodes &lt;- tvseries12episodes |&gt;\n  left_join(\n    TITLE_RATINGS,\n    join_by(tconst == tconst)\n  ) |&gt; \n  ungroup() |&gt;\n  arrange(desc(averageRating)) |&gt;\n  select( primaryTitle, averageRating) |&gt;\n  slice_head(n=10)\nhead(besttvseries12episodes,1)\n\n\n# A tibble: 1 × 2\n  primaryTitle averageRating\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Craft Games            9.7\n\n\n2.6: And lastly… is it true that episodes from later seasons of Happy Days have lower average ratings than the earlier seasons after their “Jump the shark” scene?\n\n\nCode\n#We filter out title basics for happy days to isolate and use tconst in join\njumptheshark &lt;- TITLE_BASICS |&gt;\n  filter(primaryTitle == \"Happy Days\")\n#This code joins our title episodes and ratings\njumpthesharkimpact &lt;- TITLE_EPISODES |&gt;\n  filter(parentTconst %in% jumptheshark$tconst) |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  group_by(seasonNumber) |&gt;\n  summarise(avg_rating = mean(averageRating)) |&gt;\n  arrange(seasonNumber)\n#This code creates a visualization of average rating over seasons\nggplot(jumpthesharkimpact, aes(x = seasonNumber, y = avg_rating)) +\n  geom_line(color = \"black\") + \n  geom_point(color = \"black\") +\n  labs(\n    title = \"Ratings of Happy Days by Season\",\n    x = \"Season Number\",\n    y = \"Average IMDB Rating\"\n  ) \n\n\n\n\n\n\n\n\n\nAccording to our visualization, it seems like ratings plummeted throughout the next two seasons after the “Jump the shark” episode but recovered from seasons 8-11. Despite recovering in later seasons, the earlier seasons of the show on average have higher ratings.\n\n\nCreating a Success Metric:\nAs I looked through the IMBD data sets, it made sense that the TITLE_RATINGS data set would contain the best measurable variables like average rating and number of votes.\nBased upon this, I decided to create the following success metric:\n\nSuccess Metric = (Average rating * log(Number of Votes))\n\nThis metric is a weighted score that gives more weight to movies with both high ratings and a large number of votes while also reducing the heavy right skew of our data set using a logorithmic function.\nTesting the Metric\n3.1: Choose the top 5-10 movies on your metric and confirm that they were indeed box office successes.\n\n\nCode\n#This code joins together Ratings and Title Basics to create a dataset with names and success scores\nmoviesuccess &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\") |&gt;\n  filter(titleType == \"movie\") |&gt;\n  mutate(success_score = averageRating * log10(numVotes)) |&gt;\n  arrange(desc(success_score))\n\nhead(moviesuccess %&gt;% select(primaryTitle,averageRating,numVotes, success_score), 5)\n\n\n# A tibble: 5 × 4\n  primaryTitle                              averageRating numVotes success_score\n  &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 The Shawshank Redemption                            9.3  2942823          60.2\n2 The Dark Knight                                     9    2922922          58.2\n3 The Godfather                                       9.2  2051186          58.1\n4 The Lord of the Rings: The Return of the…           9    2013824          56.7\n5 Pulp Fiction                                        8.9  2260017          56.6\n\n\nBased upon this test, I would say that our weighted success metric proves effective as all of these movies fall within the top 13 movies of all time on IMDB and were box office successes.\n3.2: Choose 3-5 movies with large numbers of IMDb votes that score poorly on your success metric and confirm that they are indeed of low quality.\n\n\nCode\n#We can filter movies with 100k+ votes and arrange based on success score\nmoviesuccess |&gt;\n  filter(numVotes &gt; 100000) |&gt;  \n  arrange(success_score) |&gt;  \n  head(5) |&gt;  \n  select(primaryTitle, averageRating, numVotes, success_score)  \n\n\n# A tibble: 5 × 4\n  primaryTitle      averageRating numVotes success_score\n  &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Radhe                       1.9   180205          9.99\n2 Epic Movie                  2.4   110222         12.1 \n3 Adipurush                   2.7   133981         13.8 \n4 Meet the Spartans           2.8   112199         14.1 \n5 365 Days                    3.3   100579         16.5 \n\n\nBased upon this test, I would also say that our weighted success metric proves effective in weighing movies that have bombed based upon large numbers of IMDb votes.\n3.3: Choose a prestige actor or director and confirm that they have many projects with high scores on your success metric.\n\n\nCode\n#This code takes our Hamill data set and joins it with movie success \nmark_hamill_movies &lt;- mark_hamill |&gt;\n  left_join(moviesuccess, by = c(\"knownForTitles\" = \"tconst\"))\n\n#This code arranges our new dataset by success score\nmark_hamill_movies |&gt;\n  arrange(desc(success_score)) |&gt;\n  head(5)|&gt;\n  select(primaryTitle, averageRating, numVotes, success_score)\n\n\n# A tibble: 4 × 4\n  primaryTitle                              averageRating numVotes success_score\n  &lt;chr&gt;                                             &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Star Wars: Episode V - The Empire Strike…           8.7  1401591          53.5\n2 Star Wars: Episode IV - A New Hope                  8.6  1471222          53.0\n3 Star Wars: Episode VI - Return of the Je…           8.3  1137692          50.3\n4 Star Wars: Episode VIII - The Last Jedi             6.9   681588          40.3\n\n\nUsing Mark Hamill’s data that we already prepped before, we can join it with our movie success data set to see that his four movies all rank rather high on our metric. (As they should being great Starwars movies!)\n3.4: Perform at least one other form of ‘spot check’ validation.\nWhat better way to do a spot-check validation than to use our metric against one of the biggest sequel flops of all time, Grease 2.\n\n\nCode\n#This code filters out for Grease 2 in primary title\nmoviesuccess |&gt;\n  filter(primaryTitle == \"Grease 2\") |&gt;\n  head(1) |&gt;\n  select(primaryTitle, averageRating, numVotes, success_score)\n\n\n# A tibble: 1 × 4\n  primaryTitle averageRating numVotes success_score\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;         &lt;dbl&gt;\n1 Grease 2               4.6    39079          21.1\n\n\nAs expected, our success score hit almost rock bottom indicating that it is effective at determining low scoring movies.\n3.5: Come up with a numerical threshold for a project to be a ‘success’; that is, determine a value v such that movies above v are all “solid” or better.\nSince Grease 2 was determined to be a flop by many standards with a success score of 21.13, using a 95% threshold at 29.82 would be reasonable to consider a project to be a success.\n\nquantile(moviesuccess$success_score, probs = seq(.9, 1, by = 0.025))\n\n     90%    92.5%      95%    97.5%     100% \n25.85849 27.53372 29.82612 33.47237 60.15951 \n\nsuccess_value &lt;- 29.82\n\n\n\nExamining Success by Genre and Decade\n4.1: What was the genre with the most “successes” in each decade?\nTo answer this question, we would first need to sort the movies into their respective genres and release dates of movies into decades.\n\n#Seperating movies with multiple genres into different lines\nmovie_genre &lt;- moviesuccess |&gt;\n  separate_rows(genres, sep = \",\")\n  \n#filtering out NA values from genres and start years\nmoviesuccess_separated &lt;- movie_genre |&gt;\n  filter(!is.na(genres) & !is.na(startYear)) |&gt; \n  mutate(decade = floor(startYear / 10) * 10) \n\nNext we can work on visualizing our new data set to get a clear view of movie preferences over the decades.\n\n\nCode\n#This code filters out succesful movies and groups them by decade then genre\n decade_success &lt;- moviesuccess_separated |&gt;\n  filter(success_score &gt;= success_value) |&gt;\n  group_by(decade, genres) |&gt;\n  summarize(\n    quantity = n(), \n    .groups = \"drop\"\n  ) |&gt;\n  group_by(decade) |&gt;\n  slice_max(quantity, n = 10, with_ties = FALSE) |&gt;\n  mutate(rank = dense_rank(desc(quantity))) |&gt;\n  ungroup() |&gt;\n  arrange(decade, rank) \n\nlibrary(randomcoloR)\nn &lt;- 20\npalette &lt;- distinctColorPalette(n)\n\n\n#This code creates a graph visualization of our data set\ndecade_success_graph&lt;-ggplot(decade_success, aes(x = decade, y = quantity, fill = genres)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Successful Movies by Decade and Genre\", x = \"Decade\", y = \"Quantity\", fill = \"Genre\") +\n  scale_fill_manual(values = distinctColorPalette(n)) +\n  theme_minimal()\ndecade_success_graph\n\n\n\n\n\n\n\n\n\n4.2: What genre consistently has the most “successes”? What genre used to reliably produce “successes” and has fallen out of favor?\nAccording to our chart, we can see that the Drama category has consistently outperformed the rest of the genres over the decades while romance seems to have fallen out for favor going from 5th place in the 2000’s to nearly last in the 2020’s.\n4.3: What genre has produced the most “successes” since 2010? Does it have the highest success rate or does it only have a large number of successes because there are many productions in that genre?\nDrama has produced the most successes since 2010 and it does seem to have a large number of successes due to production size rather than a higher success rate. In fact, dramas only have a 6% success rate compared to less popular biographies with a whopping 13%!\n\n\nCode\nmovie_genre |&gt;\n  group_by(genres) |&gt;\n  summarize(\n    Total_Movies = n(),\n    Successes = sum(success_score &gt;= success_value),\n    Success_Rate_In_Percent = round(Successes / Total_Movies * 100),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(Success_Rate_In_Percent)) |&gt;\n  DT::datatable()\n\n\n\n\n\n\n4.4: What genre has become more popular in recent years?\nIt seems as though action has been gaining traction in recent years jumping from ~4th most popular in the 90’s to ~2nd most popular in the 2020’s.\n\n\nSuccessful Personnel in the Genre\nBased on my analysis, if I were to make a successful movie I would create a biography as they historically have the highest probability of being successful. As a result, I would look to cast an actor and director who specialize in the genre.\n\n\nCode\n#Code to filter for movies that fall into the biography genre\nmoviesuccess_biography &lt;- moviesuccess_separated |&gt;\n  filter(genres == \"Biography\")\n  \n#Code to join actor names to the movies they are in and sum their success scores\nsuccessful_biography_actors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(moviesuccess_biography, by = \"tconst\") |&gt;\n  filter(category == \"actor\" | category == \"actress\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_score &gt;= success_value),\n    avg_success_score = round(mean(success_score, na.rm = TRUE),2),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n#code to join names to movies\nsuccessful_biography_actors &lt;- successful_biography_actors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_score)\n\nsuccessful_biography_actors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\nBased on my analysis of our movie data for the top biography actors, I’ve decided to select Christian Bale and Matthew McConaughey as my actors. Their ability to portray complex, morally ambiguous characters like Patrick Bateman and Mark Hanna demonstrates their suitability for a darker, more twisted biography. Given their history of success in the genre and their ability to convey nuanced emotions, I believe that they would be invaluable assets to a biographical film exploring morally complex figures in a war setting. Along with that, both of these actors fall within the top nine most successful biographical movies as well as the top four in average success score.\n\n\nCode\n#Code to filter for movies that fall into the biography genre\nmoviesuccess_biography &lt;- moviesuccess_separated |&gt;\n  filter(genres == \"Biography\")\n\n  \n#Code to join director names to the movies they are in and sum their success scores\nsuccessful_biography_directors &lt;- TITLE_PRINCIPALS |&gt;\n  inner_join(moviesuccess_biography, by = \"tconst\") |&gt;\n  filter(category == \"director\") |&gt;\n  group_by(nconst) |&gt;\n  summarise(\n    num_successful_movies = sum(success_score &gt;= success_value),\n    avg_success_score = mean(success_score, na.rm = TRUE),\n    .groups = \"drop\"\n  ) |&gt;\n  arrange(desc(num_successful_movies)) |&gt;\n  head(10)\n\n#code to join names to movies\nsuccessful_biography_directors &lt;- successful_biography_directors |&gt;\n  inner_join(NAME_BASICS, by = \"nconst\") |&gt;\n  select(primaryName, num_successful_movies, avg_success_score)\n\nsuccessful_biography_directors |&gt;\n  DT::datatable(options = list(pageLength = 5))\n\n\n\n\n\n\nIn terms of a director, there is no other clear choice to me than Steven Speilberg. Despite falling lower on the list in terms of successful biographical movies, Spielberg represents the cream of the crop in terms of average success score. Steven Spielberg’s masterful storytelling and ability to draw exceptional performances from actors would undoubtedly complement the strengths of Matthew McConaughey and Christian Bale. Spielberg’s visionary direction and keen eye for detail in works like saving Private Ryan would provide a rich, dark, and immersive world for both these talented actors to inhabit. His ability to elicit powerful emotional performances would allow McConaughey and Bale to fully explore the complexities of their characters, bringing their unique talents to the forefront.\n\n\nFinding a Classic to Remake:\nThe movie I want to remake it Schindler’s List from 1993. This movie is a historical drama film that tells the true story of Oskar Schindler,a German businessman who saved the lives of hundreds of Jews during the Holocost. Schindlers List has a whopping 9 rating on IMDB with over 1.4mm votes giving a high success score of 55.52 based on our metric. Along with that, this movie was produced over 25 years ago and having Steven Spielberg on board would make it easy to obtain the rights to reproduce it. (As he was the original director of the movie) On the other hand, our legal team would have to reach out to the former key actors like Liam Neeson and Ralph Fiennes regarding rights to the project.\n\nmoviesuccess_biography |&gt;\n  select(primaryTitle, startYear, success_score, numVotes, averageRating) |&gt;\n  DT::datatable(options = list(pageLength = 5, order = list(list(3, \"desc\"))))\n\n\n\n\n\n\n\nWrite and Deliever your Pitch\nDramas have reigned supreme in the film industry for decades and has consistently been the best performing genre for the last 100 years. With over 3400 titles created between 1990 onwards it is no surprise just how effective these movies are at evoking powerful emotions and creating heartfelt stories.\nSteven Spielberg, a visionary director with a proven track record of success, has a knack for crafting thought-provoking dramas that make people wrap heads around difficult to solve moral dilemmas. His collaboration with Matthew McConaughey and Christian Bale, two of Hollywood’s most acclaimed actors known for playing morally ambiguous roles, would create a powerful and unforgettable cinematic experience.\nSchindler’s List, a timeless tale of courage, compassion, and the human spirit, is ripe for a modern reimagining. With Spielberg’s masterful direction and the exceptional talents of McConaughey and Bale, this film promises to be a powerful exploration of relevant themes.\nThis new adaptation will captivate audiences with its compelling narrative, unforgettable performances, and stunning visuals. It’s a must-see film that will leave a lasting impression."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Do Proportional Electoral College Allocations Yield a More Representative Presidency?",
    "section": "",
    "text": "Introduction:\nThis project explores whether proportional Electoral College allocations would lead to a more representative presidency in the United States as winning without the majority vote doesn’t make sense. By examining historical election data, we will assess how alternative proportional allocation methods might impact election outcomes. The goal of this project is to determine whether reforms could make presidential elections more reflective of the nation’s diverse political preferences because, ideally, everyone’s vote should count, not just those in swing states.\nLoading our packages:\nBefore we delve any further, we will have to install the necessary r packages to visualize our data and load their respective libraries into our work space.\n\n\nCode\n#This code installs r packages if they do not exist and we need them\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"dplyr\")) install.packages(\"dplyr\")\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nif (!require(\"DT\")) install.packages(\"DT\")\nif (!require(\"sf\")) install.packages(\"sf\")\n\n\n#This loads our libraries into our environment \nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(sf)\n\n\nLoading our data sets:\nFor our analysis in this project, we will be drawing data from three different sources.\n\nUS House/Presidential Election Votes from 1976-2022\nCongressional Boundary Files from 1976-2012\nCongressional Boundary Files from 2014-Present Day\n\nUS House Election Votes from 1976-2022:\nThis data is from the MIT Election Data Science Lab and contains voter data for the US House Election Votes from 1976-2022 and the Presidential Election Votes from 1976-2022.\nAlthough we would love to automate the downloading process, these data sets are locked behind a user info screen that blocks us from doing so. To fix this, I manually downloaded them and used the following code to load them.\n\n\nCode\n#This loads our manually downloaded csv datasets \nhouse_rep_votes &lt;- readr::read_csv(\"1976-2022-house.csv\")\npresident_votes &lt;- readr::read_csv(\"1976-2020-president.csv\")\n\n\nCongressional Boundary Files from 1976-2012:\nOur next data sources are shape files for all the Congressional Boundary Files from 1976-2012 which were put together by Jeffrey Lewis, Brandon DeVine, Lincoln Pritcher, and Kenneth Martis. The following code automatically downloads these shape files for us.\n\n\nCode\nget_shapefile &lt;- function(district_name) {\n  # Construct file names\n  cmapurl1 &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  zip_filename &lt;- paste0(district_name, \".zip\")\n  shp_filename &lt;- paste0(district_name, \".shp\")\n  shapefile_path &lt;- file.path(district_name, \"districtShapes\", shp_filename)\n\n  # Download and read shapefile\n  if (!file.exists(zip_filename)) {\n    download.file(paste0(cmapurl1, zip_filename), destfile = zip_filename)\n  }\n  unzip(zipfile = zip_filename, exdir = district_name)\n  return(read_sf(shapefile_path))\n}\n\n# Download and store data (using a list)\ndistrictstart &lt;- 95\ndistrictend &lt;- 112\n\n  for (i in districtstart:districtend) {\n  district_name &lt;- sprintf(\"districts%03d\", i) \n  district_data &lt;- get_shapefile(district_name)\n  assign(district_name, district_data, envir = .GlobalEnv) \n}\n\n\nCongressional Boundary Files from 2014-Present Day:\nUnfortunately, our last data set only included district boundaries up until 2012 which is a whopping 12 years ago! Luckily for us, we can utilize data from the US Census Bureau to fill in the spaces from 2014 to the present day.\n\n\nCode\ndownloadcongress &lt;- function(fname, year){\n  # Construct file names\n  BASE_URL &lt;- sprintf(\"https://www2.census.gov/geo/tiger/TIGER%d/CD/\", year)\n  fname_ext &lt;- paste0(fname, \".zip\")\n  shapefile_path &lt;- paste0(fname, \".shp\")\n  unzip_dir &lt;- gsub(\".zip$\", \"\", fname_ext)\n  \n  # Download and unzip file if we do not have it\n  if (!file.exists(fname_ext)) {\n    download.file(paste0(BASE_URL, fname_ext), destfile = fname_ext)\n    unzip(fname_ext, exdir = unzip_dir)\n  }\n  \n  # Read and return shapefile\n  read_sf(file.path(unzip_dir, shapefile_path))\n}\n\n# Download and read district data for each year\nbase_year &lt;- 2022\nbase_congress &lt;- 116\n\nfor (i in 0:10) {\n  year &lt;- base_year - i\n  congress &lt;- ifelse(year &gt;= 2018, 116, ifelse(year &gt;= 2016, 115, ifelse(year &gt;= 2014, 114, ifelse(year == 2013, 113, 112))))\n  \n  district_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress)\n  district_data &lt;- downloadcongress(district_name, year)\n  \n  assign(district_name, district_data, envir = .GlobalEnv)\n}\n\n\nExploring Our Data Sets:\nNow that we’ve resolved our data issue and loaded the data sets, we can now begin our analysis.\nWhich states have gained and lost the most seats in the House of Representatives between 1976 and 2022?\nTo best display this data, we can create a chart that displays change in seats by state over the period from 1976 to 2022.\n\n\nCode\n# This codes filters our houserep dataset for 1976/2022 and sums seatcount\nseatcount &lt;- house_rep_votes |&gt;\n  filter(year %in% c(1976, 2022)) |&gt;\n  group_by(state, year) |&gt;\n  summarise(seat_count = n_distinct(district), .groups = \"drop\")\n\n# Code pivots data by making years to columns and seat counts into values\nseat_count_wide &lt;- pivot_wider(seatcount, names_from = year, values_from =\n                                 seat_count)\n\n# Code creates new column that calculates the difference in seat count\nseat_count_wide &lt;- seat_count_wide |&gt;\n  mutate(seat_change = `2022` - `1976`) |&gt;\n  filter(seat_change != 0)|&gt;\n  arrange(desc(seat_change))\n\n# Code creates chart to give visual aid for change in seat count by state\nggplot(seat_count_wide, aes(x = reorder(state, seat_change), y = seat_change, fill = seat_change &gt; 0)) +\n  geom_col(width = .7) + \n  scale_fill_manual(values = c(\"coral2\", \"chartreuse3\"), labels = c(\"Decrease\", \"Increase\"),name=\"Seat Change\") +\n  labs(title = \"Change in Seat Count by State (1976-2022)\",\n       x = \"State\",\n       y = \"Change in # of Seats\") +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nLooking at this chart, we can see the Texas saw the largest increase in seats while New York lost the most. To narrow in more specifically on the numbers we can use the head and tail function.\n\nhead(seat_count_wide,1)\n\n# A tibble: 1 × 4\n  state `1976` `2022` seat_change\n  &lt;chr&gt;  &lt;int&gt;  &lt;int&gt;       &lt;int&gt;\n1 TEXAS     24     38          14\n\n\n\ntail(seat_count_wide,1)\n\n# A tibble: 1 × 4\n  state    `1976` `2022` seat_change\n  &lt;chr&gt;     &lt;int&gt;  &lt;int&gt;       &lt;int&gt;\n1 NEW YORK     39     26         -13\n\n\nIt looks like Texas gained 14 seats since 1976 and New York lost 13!\nWhat is a fusion system? Does it change the outcome of elections?\nIf you live in New York, you participate in fusion voting which allows multiple political parties to endorse the same candidate. What this ultimately translates to is that a candidate can appear more than once if they are endorsed by more than one party line. While this might come off as strange, this practice enables minor parties to support major candidates while maintaining their own identities.\nAfter learning about this system, I was interested in seeing the impact it has had on prior elections in the state. By sorting our data and comparing election winners with and without fusion voting, I was surprised see that 22 elections in NY could have flipped hands if the state hadn’t used fusion voting!\n\n\nCode\n#Filter for New York\nnyelection&lt;- house_rep_votes|&gt;\n  filter(state=='NEW YORK')\n\n#Find the Election Winner\nelection_winner &lt;- nyelection |&gt;\n  group_by(year, district, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\")|&gt;\n  group_by(year, district) |&gt;\n  top_n(1, total_votes) |&gt;\n  ungroup()|&gt;\n  rename(Fusion_Winner = candidate )|&gt;\n  rename(Fusion_Votes = total_votes )\n\n\n#Find the Non-Fusion Election Winner\nnonfusionwinner &lt;- nyelection |&gt;\n  filter(party %in% c(\"DEMOCRAT\", \"REPUBLICAN\")) |&gt;\n  group_by(year, district) |&gt;\n  top_n(1, candidatevotes) |&gt;\n  ungroup() |&gt;\n  select(year, district, candidate,candidatevotes)|&gt;\n  rename(NonFusion_Winner = candidate )|&gt;\n  rename(NonFusion_Votes = candidatevotes )\n\n#Join tables for comparison\nelectioncomparison &lt;- election_winner |&gt;\n  left_join(nonfusionwinner, by = c(\"year\", \"district\")) |&gt;\n  mutate(Different_Winner = Fusion_Winner != NonFusion_Winner)|&gt;\n  filter(Different_Winner == \"TRUE\")\n\n# Datatable\ndatatable(\n  setNames(electioncomparison, c(\"Year\", \"District\", \"Fusion Winner\", \"Fusion Votes\", \"NonFusion Winner\", \"NonFusion Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"The Impact of Fusion Voting on New York House Elections\"\n)\n\n\n\n\n\n\nWhat truly astounds me about this revelation is that 54% of potential election flips occurred in the last 18 years compared to the remaining 46% from the prior 30 years of elections. Although I won’t delve into it now, it would be interesting to look into why fusion voting has become more impactful over the last two decades. (Especially given that NY has lost 13 house seats in our timeline!)\nChloropleth Visualization of the 2000 Presidential Election\nIf you were wondering when we were going to start using the shape files we downloaded earlier, you no longer need to wait! By combining our presidential votes data set with our shape files, we can put together a chloropleth visualization to help us understand how elections pan out geographically. For example, we can take a look at the 2000 presidential election between George Bush and Al Gore.\n\n\nCode\n#This code filters out our presidential data for the year 2000 \nelection2000map &lt;- president_votes |&gt;\n  filter(year == 2000) |&gt;\n  group_by(state, party_simplified) |&gt; \n  summarize(total_votes = sum(candidatevotes), .groups = \"drop\") |&gt; \n  group_by(state) |&gt; \n  top_n(1, total_votes) |&gt;\n  ungroup() |&gt; \n  select(state, party_simplified) \n\n\nmap2000 &lt;- districts106 |&gt;\n  mutate(STATENAME = toupper(trimws(STATENAME))) |&gt; \n  left_join(election2000map, by = c(\"STATENAME\" = \"state\"))  \n\n\n\nggplot(map2000) +\n  geom_sf(aes(fill = party_simplified)) +  \n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"cornflowerblue\", \"REPUBLICAN\" = \"brown2\")) +  \n  theme_minimal() +  \n  labs(\n    title = \"Presidential Election 2000 - Winning Party by State\",\n    subtitle = \"Choropleth Map of U.S. States\",\n    fill = \"Winning Party\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  coord_sf(\n    xlim = c(-180, -60),  # Adjusted to include Alaska (westward) and Hawaii (eastward)\n    ylim = c(20, 75),      # Adjusted to allow for Alaska and Hawaii's southern position\n    expand = FALSE\n  )\n\n\n\n\n\n\n\n\n\nAdvanced Chloropleth Visualization of Electoral College Results\nAnd for a better region breakdown we can look at a faceted version of the map.\n\n\nCode\nggplot(map2000) +\n  geom_sf(aes(fill = party_simplified)) +  \n  scale_fill_manual(values = c(\"DEMOCRAT\" = \"cornflowerblue\", \"REPUBLICAN\" = \"brown2\")) +  \n  theme_minimal() +  \n  labs(\n    title = \"Presidential Election 2000 - Winning Party by State\",\n    subtitle = \"Choropleth Map of U.S. States\",\n    fill = \"Winning Party\"\n  ) +\n  theme(\n    legend.position = \"bottom\",\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  coord_sf(\n    xlim = c(-180, -60),  # Adjusted to include Alaska (westward) and Hawaii (eastward)\n    ylim = c(20, 75),      # Adjusted to allow for Alaska and Hawaii's southern position\n    expand = FALSE\n  ) +\n  facet_wrap(~ party_simplified)  \n\n\n\n\n\n\n\n\n\nFairness of ECV Allocation Schemes\nCurrently, the winner-takes-all system in most states can result in presidential candidates winning the Electoral College without securing a majority of the popular vote. By utilizing our historical election data, we can assess how alternative proportional allocation methods might change these election outcomes.\nBefore we can analyze the impacts of using different voting systems, we first need calculate the number of electoral votes given to each state by adding 2 to the sum of their districts.\n\n\nCode\nelectoral_votes &lt;- house_rep_votes |&gt;\n  group_by(year, state) |&gt;\n  summarize(districts = n_distinct(district), .groups = \"drop\") |&gt;\n  mutate(electoral_votes = districts + 2) |&gt;\n  select(year, state, electoral_votes)\n\n\nState-Wide Winner-Take-All\nNow that we’re ready we can take a look at the State-Wide Winner-Take-All system where each state will allocate all of its votes to the winner of the state popular vote.\n\n\nCode\n# This code shows candidate with most votes in states\nstate_winner_take_all &lt;- president_votes |&gt;\n  group_by(year,state,candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes),\n            .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n    top_n(1, total_votes)\n\n# combining tables with electoral votes\nstate_winner_take_all &lt;- state_winner_take_all |&gt;\n  left_join(electoral_votes,\n            by = c(\"year\", \"state\")) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_ecv = sum(electoral_votes)) |&gt;\n    top_n(1, total_ecv)\n\n\n# Code Creates datatable\ndatatable(\n  setNames(state_winner_take_all, c(\"Year\", \"Elected President\", \"Electoral Votes\")),\n  options = list(pageLength = 5, autoWidth = TRUE),\n  caption = \"State-Wide Winner-Take-All\"\n)\n\n\n\n\n\n\nDistrict-Wide Winner-Take-All + State-Wide “At Large” Votes\nThis voting systems allocates electoral votes based on the winner of each district within a state which will each be rewarded one vote. The remaining votes for the state are awarded “at large” to the candidate who wins the state overall.\n\n\nCode\n#Code to find how many districts were won by each party\ndistrict_winner &lt;- house_rep_votes |&gt;\n  group_by(year, state, district) |&gt;\n  top_n(1, candidatevotes) |&gt;\n  select(year, state, district, party) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(wins = n()) \n\n# Code to find popular vote winner in the state\nat_large_winner &lt;- house_rep_votes |&gt;\n  group_by(year, state) |&gt;\n  top_n(1, candidatevotes) |&gt;\n  select(year, state, party) |&gt;\n  add_column(at_large_votes = 2)\n\n# Join tables to find electoral votes for party\nDistrictwidewinner &lt;- district_winner |&gt;\n  left_join(at_large_winner,\n    by = c(\"year\", \"state\", \"party\")\n  ) |&gt;\n  mutate(across(where(is.numeric), ~ ifelse(is.na(.), 0, .))) |&gt; \n  mutate(total_electoral_votes = wins + at_large_votes) |&gt;\n  select(-wins, -at_large_votes) |&gt;\n  rename(party_simplified = party) |&gt; \n  left_join(president_votes,\n    by = c(\"year\", \"state\", \"party_simplified\")\n  ) |&gt; \n  select(year, state, total_electoral_votes, candidate) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(electoral_votes = sum(total_electoral_votes)) |&gt;\n  top_n(1, electoral_votes) |&gt;\n  drop_na() # get rid of the non-presidential election years\n\ndatatable(\n  setNames(Districtwidewinner, c(\"Year\", \"Elected President\", \"Electoral Votes\")),\n  options = list(pageLength = 5, autoWidth = TRUE),\n  caption = \"District-Wide Winner-Take-All\"\n)\n\n\n\n\n\n\nState-Wide Proportional\nThe state-wide proportional system is where electoral votes are allocated to candidates based on the proportion of votes they receive across the entire state, rather than a winner-takes-all approach.\n\n\nCode\n# Calculate vote share, ECVs per candidate, and select the candidate with maximum ECVs per state\nstate_proportion &lt;- president_votes |&gt;\n  mutate(vote_share = round(candidatevotes / totalvotes, digits = 0)) |&gt;\n  left_join(electoral_votes, by = c(\"year\", \"state\")) |&gt;\n  mutate(ecv_per_candidate = round(vote_share * electoral_votes, digits = 0)) |&gt;\n  group_by(year, state) |&gt;\n  slice_max(ecv_per_candidate, with_ties = FALSE) |&gt;\n  ungroup() |&gt;\n  select(year, state, candidate, ecv_per_candidate)\n\n# Calculate total ECVs per candidate and find national winner per year\nnational_winner &lt;- state_proportion |&gt;\n  group_by(year, candidate) |&gt;\n  summarise(total_ecv = sum(ecv_per_candidate, na.rm = TRUE), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt;\n  ungroup()\n\n# Display the results in a data table\ndatatable(\n  setNames(national_winner, c(\"Year\", \"Candidate\", \"Total Electoral Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"State Proportional\"\n)\n\n\n\n\n\n\nNational Proportional\nLastly, a national proportional system is where electoral votes are allocated to candidates based on the proportion of the national popular vote they receive.\n\n\nCode\n# Total electoral votes per year\ntotal_ecv &lt;- electoral_votes |&gt;\n  group_by(year) |&gt;\n  summarize(total_ecv = sum(electoral_votes))\n\n# Calculate the national proportional vote share and determine the winner\nnational_proportional &lt;- president_votes |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(candidate_votes = sum(candidatevotes), .groups = \"drop\") |&gt;\n  group_by(year) |&gt;\n  mutate(percentvote = candidate_votes / sum(candidate_votes)) |&gt;\n  left_join(total_ecv, by = \"year\") |&gt;\n  mutate(ecv_received = round(percentvote * total_ecv)) |&gt;\n  group_by(year) |&gt;\n  slice_max(ecv_received, n = 1, with_ties = FALSE) |&gt;\n  rename(winner = candidate) |&gt;\n  select(year, winner, ecv_received)\n\n# Display the results in a data table\ndatatable(\n  setNames(national_proportional, c(\"Year\", \"Elected President\", \"Electoral Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE),\n  caption = \"National Proportional\"\n)\n\n\n\n\n\n\nConclusion:\nUltimately, the national proportional strategy emerges as the “fairest” method because it best reflects the interests of the entire national voting population. This method equally weighs individual votes across the country, regardless of variations in states, and truly represents a popular vote. The most recent example where this method would have been useful was during the 2016 Presidential election where Donald Trump won in total electoral college votes despite Clinton winning the popular vote by ~3m votes (2.1%).\nThe next best method is the state-wide proportional strategy. This method is similar to the national proportional method but on a more granular state level. While it weighs each voters opinion equally within each state, it’s less fair than the national proportional method because certain states will have a greater impact than others. (I.E Swing states)\nLastly, I would rank both the state-wide winner-take-all and district-wide winner-take-all methods as the “least fair” approaches to presidential elections. Not only do they inaccurately reflect the nation’s collective interests, but they are also subject to biases due to varying demographics and population distributions. Both of these methods often amplify the voices of specific regions or groups which can overshadowing diverse perspectives within larger states or across districts. Along with that, these methods create incentives for candidates to focus heavily on swing states or particular regions rather than on representing the broader national interest. This can mislead the electoral process, making it less representative and diminishing voter influence in “safe” states or districts, ultimately leading to an unbalanced reflection of public opinion.\nCitations:\nJeffrey B. Lewis, Brandon DeVine, Lincoln Pitcher, and Kenneth C. Martis. (2013) Digital Boundary Definitions of United States Congressional Districts, 1789-2012. [Data file and code book]. Retrieved from https://cdmaps.polisci.ucla.edu on [October 30, 2024].\nMIT Election Data and Science Lab, 2017, “U.S. House 1976–2022”, https://doi.org/10.7910/DVN/IG0UN2, Harvard Dataverse, V13, UNF:6:Ky5FkettbvohjTSN/IVldA== [fileUNF]\nMIT Election Data and Science Lab, 2017, “U.S. President 1976–2020”, https://doi.org/10.7910/DVN/42MVDX, Harvard Dataverse, V8, UNF:6:F0opd1IRbeYI9QyVfzglUw== [fileUNF]"
  }
]